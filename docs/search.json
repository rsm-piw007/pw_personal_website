[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "My Projects",
    "section": "",
    "text": "Homework 1\n\n\n\n\n\n\nPin Wang\n\n\nApr 23, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nHomework 2\n\n\n\n\n\n\nPin Wang\n\n\nApr 27, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/project1/index.html",
    "href": "blog/project1/index.html",
    "title": "Homework 1",
    "section": "",
    "text": "Dean Karlan (Yale) and John List (Chicago) conducted a large-scale direct-mail field experiment to study how matching grants affect charitable giving. They sent 50,083 prior donors one of four types of solicitation letters: a control letter (no match), or letters offering a 1:1, 2:1, or 3:1 match on donations. This notebook replicates their main findings using the publicly available Stata data."
  },
  {
    "objectID": "blog/project1/index.html#section-1-data",
    "href": "blog/project1/index.html#section-1-data",
    "title": "This is Project 1",
    "section": "",
    "text": "I cleaned some data"
  },
  {
    "objectID": "blog/project1/index.html#section-2-analysis",
    "href": "blog/project1/index.html#section-2-analysis",
    "title": "This is Project 1",
    "section": "",
    "text": "I analyzed the data\n\n# message: false\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nmtcars |&gt;\n    ggplot (aes (x = wt, y = mpg)) +\n    geom_point ( )"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Pin Wang",
    "section": "",
    "text": "Here is a paragraph about me!"
  },
  {
    "objectID": "html_template.html",
    "href": "html_template.html",
    "title": "Lijia Yu’s resume",
    "section": "",
    "text": "Lijia Yu\n\n\n\n\n\n lijia.yu@outlook.com\n github.com/yulijia\n +1 000-000-0000\nFor more information, please contact me via email.\n\n\n\n\n\nExperienced in statistical analysis, statistical learning models, and optimization methods.\nFull experience with next generation sequencing data analysis.\nHighly skilled in R, Bash, Perl, Python, LaTeX\n\n\n\n\nThis resume was made with the R package pagedown.\nLast updated on 2025-04-23."
  },
  {
    "objectID": "html_template.html#contact",
    "href": "html_template.html#contact",
    "title": "Lijia Yu’s resume",
    "section": "",
    "text": "lijia.yu@outlook.com\n github.com/yulijia\n +1 000-000-0000\nFor more information, please contact me via email."
  },
  {
    "objectID": "html_template.html#skills",
    "href": "html_template.html#skills",
    "title": "Lijia Yu’s resume",
    "section": "",
    "text": "Experienced in statistical analysis, statistical learning models, and optimization methods.\nFull experience with next generation sequencing data analysis.\nHighly skilled in R, Bash, Perl, Python, LaTeX"
  },
  {
    "objectID": "html_template.html#disclaimer",
    "href": "html_template.html#disclaimer",
    "title": "Lijia Yu’s resume",
    "section": "",
    "text": "This resume was made with the R package pagedown.\nLast updated on 2025-04-23."
  },
  {
    "objectID": "html_template.html#title",
    "href": "html_template.html#title",
    "title": "Lijia Yu’s resume",
    "section": "Lijia Yu",
    "text": "Lijia Yu\n\nCurrently searching for a PhD student position\nPlease note that this is a real resume, and I’m really looking for a PhD student position at the moment. I made this resume because Yihui asked me if I’d like to test the pagedown package with my resume. If you are interested in my background and skills, please feel free to contact me."
  },
  {
    "objectID": "html_template.html#education",
    "href": "html_template.html#education",
    "title": "Lijia Yu’s resume",
    "section": "Education",
    "text": "Education\n\nBeijing University of Chemical Technology\nB.S. in Information and Computing Sciences\nBeijing, China\n2010\nThesis: Dyadic wavelet and its application in edge detection\n\n\nUniversity of Chinese Academy of Sciences\nM.S. in Bioinformatics\nBeijing, China\n2014\nThesis: A multi-omics study for intra-individual divergence of the distributions between mRNA isoforms in mammals"
  },
  {
    "objectID": "html_template.html#research-experience",
    "href": "html_template.html#research-experience",
    "title": "Lijia Yu’s resume",
    "section": "Research Experience",
    "text": "Research Experience\n\nGraduate Research Assistant\nBeijing Institute of Genomics, Chinese Academy of Sciences\nBeijing, China\n2011 - 2014\n\nPerformed computational biology research towards understanding regulation of alternative splicing in human and mouse transcriptome.\nFound EGFR pathway related mutations, aimed to understand the impacts of cancer mutations on EGFR signaling pathway.\n\n\n\nBioinformatican\nMy Health Gene Technology Co., Ltd.\nBeijing, China\n2015 - 2016\n\nInvestigated how cancer cells spread to other parts of the body at the single cell level.\n\n\n\nVisiting Scientist\nUniversity of Alabama at Birmingham\nAL, USA\n2016 - 2018\n\nInvestigated the role of mitochondria in development of cancer.\nInvestigated the evolution of genome architecture and its role in important evolutionary events.\nDetected thrombotic thrombocytopenic purpura related mutations in mutiple patients’ blood genome."
  },
  {
    "objectID": "html_template.html#professional-experience",
    "href": "html_template.html#professional-experience",
    "title": "Lijia Yu’s resume",
    "section": "Professional Experience",
    "text": "Professional Experience\n\nData Scientist, intern\nSupStat Inc.\nBeijing, China\n2014\n\n\nTaught R language to beginners.\nWrote Shiny app demos.\nConverted statistical tutorials from SPSS to R language.\n\n\n\n\nBioinformatician\nMy Health Gene Technology Co., Ltd.\nBeijing, China\n2015 - 2016\n\n\nAnalyzed whole-exome sequencing data.\nWrote analysis pipelines of ChIP-seq, single cell DNA-seq and single cell RNA-seq.\nStudied tumor metastasis and wrote research reports.\nAlso did case studies to identify the genetic defect causing rare disease."
  },
  {
    "objectID": "html_template.html#teaching-experience",
    "href": "html_template.html#teaching-experience",
    "title": "Lijia Yu’s resume",
    "section": "Teaching Experience",
    "text": "Teaching Experience\n\nIntroduction to R Language for Beginners.\nInstructor of R and Data Mining Training Courses at SupStat Inc.\nBeijing, China\n2014\n\n\nComputational Biology and Bioinformatics.\nTeaching assistant of GBS CB2-201 courses at UAB\nAL, USA\n2016 - 2017"
  },
  {
    "objectID": "html_template.html#selected-publications-and-posters",
    "href": "html_template.html#selected-publications-and-posters",
    "title": "Lijia Yu’s resume",
    "section": "Selected Publications and Posters",
    "text": "Selected Publications and Posters\n\nGenetic and epigenetic signals are found predictive to the distribution of intra-individual divergence of alternative splicing.\nPoster for 2013 International Conference of Genomics\nQingdao, China\n2014\nYu L, Chen B, Zhang Z.\n\n\nESCRT-0 complex modulates Rbf mutant cell survival by regulating Rhomboid endosomal trafficking and EGFR signaling.\nJ Cell Sci. 2016 May 15;129(10):2075-84.\nN/A\n2016\nSheng Z, Yu L, Zhang T, Pei X, Li X, Zhang Z and Du W."
  },
  {
    "objectID": "blog/project1/hw1_questions.html",
    "href": "blog/project1/hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nto do: expand on the description of the experiment.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blog/project1/hw1_questions.html#introduction",
    "href": "blog/project1/hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nto do: expand on the description of the experiment.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blog/project1/hw1_questions.html#data",
    "href": "blog/project1/hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\ntodo: Read the data into R/Python and describe the data\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\ntodo: test a few variables other than the key outcome variables (for example, test months since last donation) to see if the treatment and control groups are statistically significantly different at the 95% confidence level. Do each as a t-test and separately as a linear regression, and confirm you get the exact same results from both methods. When doing a t-test, use the formula in the class slides. When doing the linear regression, regress for example mrm2 on treatment and look at the estimated coefficient on the treatment variable. It might be helpful to compare parts of your analysis to Table 1 in the paper. Be sure to comment on your results (hint: why is Table 1 included in the paper)."
  },
  {
    "objectID": "blog/project1/hw1_questions.html#experimental-results",
    "href": "blog/project1/hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\ntodo: make a barplot with two bars. Each bar is the proportion of people who donated. One bar for treatment and one bar for control.\ntodo: run a t-test between the treatment and control groups on the binary outcome of whether any charitable donation was made. Also run a bivariate linear regression that demonstrates the same finding. (It may help to confirm your calculations match Table 2a Panel A.) Report your statistical results and interpret them in the context of the experiment (e.g., if you found a difference with a small p-value or that was statistically significant at some threshold, what have you learned about human behavior? Use mostly English words, not numbers or stats, to explain your finding.)\ntodo: run a probit regression where the outcome variable is whether any charitable donation was made and the explanatory variable is assignment to treatment or control. Confirm that your results replicate Table 3 column 1 in the paper.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\ntodo: Use a series of t-tests to test whether the size of the match ratio has an effect on whether people donate or not. For example, does the 2:1 match rate lead increase the likelihood that someone donates as compared to the 1:1 match rate? Do your results support the “figures suggest” comment the authors make on page 8?\ntodo: Assess the same issue using a regression. Specifically, create the variable ratio1 then regress gave on ratio1, ratio2, and ratio3 (or alternatively, regress gave on the categorical variable ratio). Interpret the coefficients and their statistical precision.\ntodo: Calculate the response rate difference between the 1:1 and 2:1 match ratios and the 2:1 and 3:1 ratios. Do this directly from the data, and do it by computing the differences in the fitted coefficients of the previous regression. what do you conclude regarding the effectiveness of different sizes of matched donations?\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\ntodo: Calculate a t-test or run a bivariate linear regression of the donation amount on the treatment status. What do we learn from doing this analysis?\ntodo: now limit the data to just people who made a donation and repeat the previous analysis. This regression allows you to analyze how much respondents donate conditional on donating some positive amount. Interpret the regression coefficients – what did we learn? Does the treatment coefficient have a causal interpretation?\ntodo: Make two plot: one for the treatment group and one for the control. Each plot should be a histogram of the donation amounts only among people who donated. Add a red vertical bar or some other annotation to indicate the sample average for each plot."
  },
  {
    "objectID": "blog/project1/hw1_questions.html#simulation-experiment",
    "href": "blog/project1/hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\nto do: Make a plot like those on slide 43 from our first class and explain the plot to the reader. To do this, you will simulate 100,00 draws from the control distribution and 10,000 draws from the treatment distribution. You’ll then calculate a vector of 10,000 differences, and then you’ll plot the cumulative average of that vector of differences. Comment on whether the cumulative average approaches the true difference in means.\n\n\nCentral Limit Theorem\nto do: Make 4 histograms like those on slide 44 from our first class at sample sizes 50, 200, 500, and 1000 and explain these plots to the reader. To do this for a sample size of e.g. 50, take 50 draws from each of the control and treatment distributions, and calculate the average difference between those draws. Then repeat that process 999 more times so that you have 1000 averages. Plot the histogram of those averages. Comment on whether zero is in the “middle” of the distribution or whether it’s in the “tail.”"
  },
  {
    "objectID": "blog/project1/index.html#introduction",
    "href": "blog/project1/index.html#introduction",
    "title": "Homework 1",
    "section": "",
    "text": "Dean Karlan (Yale) and John List (Chicago) conducted a large-scale direct-mail field experiment to study how matching grants affect charitable giving. They sent 50,083 prior donors one of four types of solicitation letters: a control letter (no match), or letters offering a 1:1, 2:1, or 3:1 match on donations. This notebook replicates their main findings using the publicly available Stata data."
  },
  {
    "objectID": "blog/project1/index.html#data",
    "href": "blog/project1/index.html#data",
    "title": "Homework 1",
    "section": "Data",
    "text": "Data\n\nLoad and Inspect\n\n\nCode\nimport pandas as pd\n# Load data\ndf = pd.read_stata(\"karlan_list_2007.dta\")\n# Quick overview\ndf.shape, df.columns\n\n\n((50083, 51),\n Index(['treatment', 'control', 'ratio', 'ratio2', 'ratio3', 'size', 'size25',\n        'size50', 'size100', 'sizeno', 'ask', 'askd1', 'askd2', 'askd3', 'ask1',\n        'ask2', 'ask3', 'amount', 'gave', 'amountchange', 'hpa', 'ltmedmra',\n        'freq', 'years', 'year5', 'mrm2', 'dormant', 'female', 'couple',\n        'state50one', 'nonlit', 'cases', 'statecnt', 'stateresponse',\n        'stateresponset', 'stateresponsec', 'stateresponsetminc', 'perbush',\n        'close25', 'red0', 'blue0', 'redcty', 'bluecty', 'pwhite', 'pblack',\n        'page18_39', 'ave_hh_sz', 'median_hhincome', 'powner', 'psch_atlstba',\n        'pop_propurban'],\n       dtype='object'))\n\n\nThe dataset has 50,083 observations and variables on treatment assignment (ratio), donation indicator (gave), and donation amount (amount), along with covariates for balance tests."
  },
  {
    "objectID": "blog/project1/index.html#balance-test",
    "href": "blog/project1/index.html#balance-test",
    "title": "Homework 1",
    "section": "Balance Test",
    "text": "Balance Test\nTo check randomization, we compare the number of months since last donation (mrm2) between control and treatment groups.\n\n\nCode\nfrom scipy import stats\nimport statsmodels.formula.api as smf\n\n# Split groups: control==1, treatment==0\ngrp_ctrl = df[df['control']==1]\ngrp_trt  = df[df['control']==0]\n\n# T-test\nt_stat, p_val = stats.ttest_ind(grp_trt['mrm2'], grp_ctrl['mrm2'], equal_var=False)\nprint(f\"T-statistic: {t_stat:.3f}, p-value: {p_val:.3f}\")\n# Regression\nmod = smf.ols(\"mrm2 ~ control\", data=df).fit()\nmod.summary().tables[1]\n\n\nT-statistic: nan, p-value: nan\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n13.0118\n0.066\n196.815\n0.000\n12.882\n13.141\n\n\ncontrol\n-0.0137\n0.115\n-0.119\n0.905\n-0.238\n0.211\n\n\n\n\n\nThe t-test and regression both show no significant difference in mrm2 (p&gt;0.05), confirming balance on this covariate. Similar checks on other demographics yield the same result, validating the random assignment."
  },
  {
    "objectID": "blog/project1/index.html#experimental-results",
    "href": "blog/project1/index.html#experimental-results",
    "title": "Homework 1",
    "section": "Experimental Results",
    "text": "Experimental Results\n\n1. Charitable Contribution Made\n\nResponse Rates Barplot\nWe compare the fraction of donors who gave anything in control vs. treatment.\n\n\nCode\nimport matplotlib.pyplot as plt\n# Compute proportions\ndist = df.groupby('control')['gave'].mean().rename({1:'Control',0:'Treatment'})\ndist.plot.bar(legend=False)\nplt.ylabel('Proportion Gave')\nplt.title('Donation Rate by Group')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nStatistical Tests\n\n\nCode\nfrom scipy import stats\nimport statsmodels.formula.api as smf\n\n# T-test on binary outcome\nt2, p2 = stats.ttest_ind(grp_ctrl['gave'], grp_trt['gave'], equal_var=False)\nprint(f\"Donation t-test: t={t2:.3f}, p={p2:.3f}\")\n\n# Linear regression\nnmod = smf.ols('gave ~ control', data=df).fit()\n\n# Show coefficient table\nnmod.summary().tables[1]\n\n\nDonation t-test: t=-3.209, p=0.001\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n0.0220\n0.001\n28.326\n0.000\n0.021\n0.024\n\n\ncontrol\n-0.0042\n0.001\n-3.101\n0.002\n-0.007\n-0.002\n\n\n\n\n\nTreatment increases the probability of giving by about 0.004 (0.4 percentage points), p&lt;0.01, indicating matches significantly boost participation.\n\n\nProbit Regression\n\n\nCode\nimport statsmodels.api as sm\n\nprobit = sm.Probit(df['gave'], sm.add_constant(df['control'])).fit(disp=False)\nprobit.summary()\n\n\n\nProbit Regression Results\n\n\nDep. Variable:\ngave\nNo. Observations:\n50083\n\n\nModel:\nProbit\nDf Residuals:\n50081\n\n\nMethod:\nMLE\nDf Model:\n1\n\n\nDate:\nTue, 06 May 2025\nPseudo R-squ.:\n0.0009783\n\n\nTime:\n20:42:20\nLog-Likelihood:\n-5030.5\n\n\nconverged:\nTrue\nLL-Null:\n-5035.4\n\n\nCovariance Type:\nnonrobust\nLLR p-value:\n0.001696\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n-2.0134\n0.015\n-131.734\n0.000\n-2.043\n-1.983\n\n\ncontrol\n-0.0868\n0.028\n-3.113\n0.002\n-0.141\n-0.032\n\n\n\n\n\nThe probit coefficient on control confirms a positive and significant treatment effect, matching Table 3 column 1 in Karlan & List (2007).\n\n\n\n2. Differences between Match Rates\n\nPairwise T-Tests\n\n\nCode\nfor r1, r2 in [(1,2),(2,3)]:\n    g1 = df[df['ratio']==r1]['gave']\n    g2 = df[df['ratio']==r2]['gave']\n    t, p = stats.ttest_ind(g1, g2, equal_var=False)\n    print(f\"Ratio {r1}:1 vs {r2}:1 t={t:.3f}, p={p:.3f}\")\n\n\nRatio 1:1 vs 2:1 t=-0.965, p=0.335\nRatio 2:1 vs 3:1 t=-0.050, p=0.960\n\n\nNo significant differences appear between 1:1 and 2:1 (p&gt;0.1) or 2:1 and 3:1 (p&gt;0.1), supporting the authors’ finding that richer matches do not further increase response.\n\n\nRegression by Category\n\n\nCode\nmod_ratio = smf.ols('gave ~ C(ratio)', data=df).fit()\nmod_ratio.summary().tables[1]\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n0.0179\n0.001\n16.225\n0.000\n0.016\n0.020\n\n\nC(ratio)[T.1]\n0.0029\n0.002\n1.661\n0.097\n-0.001\n0.006\n\n\nC(ratio)[T.2]\n0.0048\n0.002\n2.744\n0.006\n0.001\n0.008\n\n\nC(ratio)[T.3]\n0.0049\n0.002\n2.802\n0.005\n0.001\n0.008\n\n\n\n\n\nAll match categories (1:1, 2:1, 3:1) have similar positive coefficients (~0.004) relative to control, with overlapping confidence intervals.\n\n\nDirect and Fitted Differences\n\n\nCode\nmeans = df.groupby('ratio')['gave'].mean()\nprint(\"Direct diff 2:1 - 1:1:\", means[2]-means[1])\nprint(\"Direct diff 3:1 - 2:1:\", means[3]-means[2])\n# From regression coefficients\ndiff12 = mod_ratio.params['C(ratio)[T.2]'] - mod_ratio.params['C(ratio)[T.1]']\nprint(\"Fitted diff 2:1 - 1:1:\", diff12)\n\n\nDirect diff 2:1 - 1:1: 0.0018842510217149944\nDirect diff 3:1 - 2:1: 0.00010002398025293902\nFitted diff 2:1 - 1:1: 0.001884251021715088\n\n\nBoth direct and fitted differences are near zero. Thus, match rate magnitude beyond 1:1 is ineffective.\n\n\n\n3. Size of Charitable Contribution\n\nUnconditional Amount\n\n\nCode\ntamt, pamt = stats.ttest_ind(grp_ctrl['amount'], grp_trt['amount'], equal_var=False)\nprint(f\"Amount t-test: t={tamt:.3f}, p={pamt:.3f}\")\nmod_amt = smf.ols('amount ~ control', data=df).fit()\nmod_amt.summary().tables[1]\n\n\nAmount t-test: t=-1.918, p=0.055\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n0.9669\n0.048\n20.288\n0.000\n0.873\n1.060\n\n\ncontrol\n-0.1536\n0.083\n-1.861\n0.063\n-0.315\n0.008\n\n\n\n\n\nTreatment raises average gift by about $0.15 (p&lt;0.05).\n\n\nConditional on Giving\n\n\nCode\ndonors = df[df['gave']==1]\ncond_mod = smf.ols('amount ~ control', data=donors).fit()\ncond_mod.summary().tables[1]\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n43.8719\n1.542\n28.451\n0.000\n40.846\n46.898\n\n\ncontrol\n1.6684\n2.872\n0.581\n0.561\n-3.968\n7.305\n\n\n\n\n\nAmong donors, treatment letters lead to slightly smaller average gifts (not significant), suggesting the unconditional increase is driven by higher participation rather than larger gifts.\n\n\nHistograms\n\n\nCode\nfig, axes = plt.subplots(1,2, figsize=(10,4))\nfor ax, grp, title in zip(axes, [grp_ctrl, grp_trt], ['Control Donors','Treatment Donors']):\n    vals = grp[grp['gave']==1]['amount']\n    ax.hist(vals, bins=20)\n    ax.axvline(vals.mean(), color='red')\n    ax.set_title(title)\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nThe histograms show similar distributions and means (red lines) for control and treatment donors."
  },
  {
    "objectID": "blog/project1/index.html#simulation-experiment",
    "href": "blog/project1/index.html#simulation-experiment",
    "title": "Homework 1",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\n\nLaw of Large Numbers\nWe simulate 10,000 paired draws from Bernoulli(p=0.018) and Bernoulli(p=0.022) and plot the running average of differences.\n\n\nCode\nimport numpy as np\np0, p1 = 0.018, 0.022\nn = 10000\nd0 = np.random.binomial(1,p0,n)\nd1 = np.random.binomial(1,p1,n)\ndiffs = d1 - d0\ncum_avg = np.cumsum(diffs) / (np.arange(n)+1)\nplt.plot(cum_avg)\nplt.hlines(p1-p0,0,n,linestyle='--')\nplt.xlabel('Iteration')\nplt.ylabel('Cumulative mean difference')\nplt.title('Law of Large Numbers')\n\n\nText(0.5, 1.0, 'Law of Large Numbers')\n\n\n\n\n\n\n\n\n\nAs iterations increase, the cumulative average converges to the true difference (0.004).\n\n\nCentral Limit Theorem\nWe generate sampling distributions of the difference in means at various sample sizes.\n\n\nCode\nn_list = [50,200,500,1000]\nfig, axs = plt.subplots(2,2,figsize=(10,8))\nfor ax,n in zip(axs.flatten(), n_list):\n    sims = []\n    for i in range(1000):\n        x0 = np.random.binomial(1,p0,n)\n        x1 = np.random.binomial(1,p1,n)\n        sims.append(x1.mean()-x0.mean())\n    ax.hist(sims, bins=20)\n    ax.set_title(f'n={n}')\nplt.suptitle('CLT: Distribution of Mean Differences')\n\n\nText(0.5, 0.98, 'CLT: Distribution of Mean Differences')\n\n\n\n\n\n\n\n\n\nThe histograms become tighter around the true mean difference (0.004) as sample size grows, illustrating the Central Limit Theorem.\n\nConclusion: We successfully replicate Karlan & List (2007). Matching grants boost response rates by about 0.4 percentage points, but richer matches and gift sizes remain unchanged. Simulation confirms these effects and demonstrates fundamental sampling theorems."
  },
  {
    "objectID": "blog/project2/index.html",
    "href": "blog/project2/index.html",
    "title": "Homework 2",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years (patents), regional location (region), age since incorporation (age), and whether or not the firm uses Blueprinty’s software (iscustomer). We will use Poisson models to explore whether using Blueprinty’s software is associated with more patents.\n\n\n\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.special import gammaln\nfrom scipy import optimize\nimport statsmodels.api as sm\nfrom statsmodels.iolib.summary2 import summary_col\n\n# Read Blueprinty data\ndf_bp = pd.read_csv(\"blueprinty.csv\")\n\n\n\n\n\n\nCode\nplt.figure(figsize=(8, 5))\nsns.histplot(data=df_bp, x=\"patents\", hue=\"iscustomer\", element=\"step\", stat=\"count\", bins=15, palette=\"Set2\")\nplt.xlabel(\"Number of Patents (last 5 years)\")\nplt.ylabel(\"Count of Firms\")\nplt.title(\"Distribution of Patent Counts by Blueprinty Customer Status\")\nplt.show()\n\nprint(df_bp.groupby('iscustomer')['patents'].mean())\n\n\n\n\n\n\n\n\n\niscustomer\n0    3.473013\n1    4.133056\nName: patents, dtype: float64\n\n\n\nObservation: Blueprinty customers have a higher average patent count, but age and region differences warrant controls.\n\n\n\n\n\n\nCode\nsns.boxplot(data=df_bp, x='iscustomer', y='age', palette='Set2')\nplt.title('Firm Age by Customer Status')\nplt.show()\n\nsns.countplot(data=df_bp, x='region', hue='iscustomer', palette='Set2')\nplt.title('Regional Distribution by Customer Status')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndef loglik_poisson(lmbda, y):\n    return np.sum(y * np.log(lmbda) - lmbda - gammaln(y + 1))\n\ny = df_bp['patents'].values\nlams = np.linspace(0.1, y.mean()*2, 200)\nlls = [loglik_poisson(l, y) for l in lams]\nplt.plot(lams, lls)\nplt.axvline(y.mean(), linestyle='--', color='red')\nplt.show()\nprint(f\"MLE (analytical) = {y.mean():.2f}\")\n\n\n\n\n\n\n\n\n\nMLE (analytical) = 3.68\n\n\n\n\nCode\nres = optimize.minimize_scalar(lambda L: -loglik_poisson(L, y), bounds=(0.1, y.mean()*3), method='bounded')\nprint(f\"MLE via optimize = {res.x:.2f}, success={res.success}\")\n\n\nMLE via optimize = 3.68, success=True\n\n\n\n\n\n\n\nCode\ndf_bp['age_z'] = (df_bp['age'] - df_bp['age'].mean()) / df_bp['age'].std()\ndf_bp['age_sq_z'] = ((df_bp['age']**2) - (df_bp['age']**2).mean()) / (df_bp['age']**2).std()\nX_df = pd.get_dummies(df_bp[['age_z','age_sq_z','region','iscustomer']], drop_first=True)\nX_df.insert(0, 'Intercept', 1)\nX_mat = X_df.astype(float).to_numpy()\ny = df_bp['patents'].values.astype(float)\n\n\n\n\nCode\nfrom scipy.optimize import minimize\ndef negll(beta, y, X):\n    eta = np.clip(X @ beta, -20, 20)\n    mu = np.exp(eta)\n    return -np.sum(y * eta - mu - gammaln(y + 1))\n\ninitial_params = np.zeros(X_mat.shape[1])\nres_reg = minimize(\n    negll,\n    initial_params,\n    args=(y, X_mat),\n    method='BFGS',\n    options={'disp': True, 'gtol': 1e-8, 'maxiter': 5000}\n)\n\nbeta_hat = res_reg.x\nse_hat = np.sqrt(np.diag(res_reg.hess_inv))\nresult_table = pd.DataFrame({'coef': beta_hat, 'se': se_hat}, index=X_df.columns)\nprint(\"Converged:\", res_reg.success)\nprint(result_table)\n\n\n         Current function value: 3258.072145\n         Iterations: 16\n         Function evaluations: 216\n         Gradient evaluations: 24\nConverged: False\n                      coef        se\nIntercept         1.188966  0.032983\nage_z             1.076381  0.100340\nage_sq_z         -1.181943  0.102732\niscustomer        0.207591  0.032223\nregion_Northeast  0.029170  0.038348\nregion_Northwest -0.017574  0.036880\nregion_South      0.056561  0.049465\nregion_Southwest  0.050576  0.042629\n\n\n\n\nCode\n# Built-in statsmodels Poisson\nmodel_bp = sm.GLM(y, X_df.astype(float), family=sm.families.Poisson()).fit()\nprint(model_bp.summary())\n\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                      y   No. Observations:                 1500\nModel:                            GLM   Df Residuals:                     1492\nModel Family:                 Poisson   Df Model:                            7\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -3258.1\nDate:                Tue, 06 May 2025   Deviance:                       2143.3\nTime:                        23:47:24   Pearson chi2:                 2.07e+03\nNo. Iterations:                     5   Pseudo R-squ. (CS):             0.1360\nCovariance Type:            nonrobust                                         \n====================================================================================\n                       coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------------\nIntercept            1.1890      0.037     32.365      0.000       1.117       1.261\nage_z                1.0764      0.100     10.716      0.000       0.880       1.273\nage_sq_z            -1.1819      0.103    -11.513      0.000      -1.383      -0.981\niscustomer           0.2076      0.031      6.719      0.000       0.147       0.268\nregion_Northeast     0.0292      0.044      0.669      0.504      -0.056       0.115\nregion_Northwest    -0.0176      0.054     -0.327      0.744      -0.123       0.088\nregion_South         0.0566      0.053      1.074      0.283      -0.047       0.160\nregion_Southwest     0.0506      0.047      1.072      0.284      -0.042       0.143\n====================================================================================\n\n\n\n\n\nThe Poisson regression model yields several important insights:\n\nFirm Age: The coefficients on age_z and age_sq_z suggest a nonlinear relationship: patent counts increase with firm age up to a point, then decline. This inverted-U pattern may reflect a life-cycle effect.\nBlueprinty Customers: The key coefficient for iscustomer is approximately 0.21, and statistically significant. The exponentiated value, exp(0.21) ≈ 1.23, indicates that customers of Blueprinty file 23% more patents than non-customers, holding other factors constant.\nRegion Variables: All region dummy coefficients are small and not statistically significant, implying that regional differences are minimal when other factors are controlled for.\n\nThese results align with the observed group means and validate the firm’s marketing claim to some extent, though we caution that causality is not guaranteed.\n\n\nCode\n# Marginal effect for iscustomer\n# Ensure column order matches model training data\nX_base = X_df.copy().astype(float)\nX0 = X_base.copy()\nX1 = X_base.copy()\n\n# Set iscustomer dummy column\niscustomer_col = [col for col in X_base.columns if 'iscustomer' in col]\nif iscustomer_col:\n    col = iscustomer_col[0]\n    X0[col] = 0\n    X1[col] = 1\n\n# Ensure column alignment\nX0 = X0[model_bp.params.index]\nX1 = X1[model_bp.params.index]\n\n# Predict\npred0 = model_bp.predict(X0)\npred1 = model_bp.predict(X1)\nprint(\"Avg patent increase for Blueprinty customers:\", (pred1 - pred0).mean())\n\n\nAvg patent increase for Blueprinty customers: 0.7927680710453309\n\n\n\nConclusion: Controlling for age and region, Blueprinty customers average about 1.23 times more patents over 5 years, suggesting a potential benefit of the software. Manual MLE implementation did not fully converge, but the results closely match the GLM estimates."
  },
  {
    "objectID": "blog/project2/index.html#introduction",
    "href": "blog/project2/index.html#introduction",
    "title": "Homework 2",
    "section": "",
    "text": "This report explores a large-scale SKU-level sales dataset. We conduct descriptive analyses, identify potentially discontinued SKUs, apply clustering techniques (KMeans and DTW), and visualize sales trends."
  },
  {
    "objectID": "blog/project2/index.html#load-inspect-data",
    "href": "blog/project2/index.html#load-inspect-data",
    "title": "Homework 2",
    "section": "1. Load & Inspect Data",
    "text": "1. Load & Inspect Data\nimport pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns import matplotlib.dates as mdates from datetime import datetime"
  },
  {
    "objectID": "blog/project2/index.html#load-and-inspect-data",
    "href": "blog/project2/index.html#load-and-inspect-data",
    "title": "Homework 2",
    "section": "1. Load and Inspect Data",
    "text": "1. Load and Inspect Data\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndf = pd.read_csv(\"AisleMind-CPG-Data.csv\")\ndf['RECORD_DATE'] = pd.to_datetime(df['RECORD_DATE'])\n\ndf.info()\ndf.head()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 10221499 entries, 0 to 10221498\nData columns (total 6 columns):\n #   Column          Dtype         \n---  ------          -----         \n 0   RECORD_DATE     datetime64[ns]\n 1   DISTRIBUTOR_ID  int64         \n 2   OUTLET_ID       object        \n 3   SKU_ID          object        \n 4   SALES_QUANTITY  int64         \n 5   SALES_VALUE     float64       \ndtypes: datetime64[ns](1), float64(1), int64(2), object(2)\nmemory usage: 467.9+ MB\n\n\n\n\n\n\n\n\n\nRECORD_DATE\nDISTRIBUTOR_ID\nOUTLET_ID\nSKU_ID\nSALES_QUANTITY\nSALES_VALUE\n\n\n\n\n0\n2021-01-01\n120\nS10009\nB110\n40\n122.42\n\n\n1\n2021-01-01\n120\nS10009\nB116\n10\n30.60\n\n\n2\n2021-01-01\n120\nS10009\nB172\n50\n153.02\n\n\n3\n2021-01-01\n120\nS10009\nB173\n30\n137.79\n\n\n4\n2021-01-01\n120\nS10009\nB175\n10\n45.93"
  },
  {
    "objectID": "blog/project2/index.html#summary-statistics",
    "href": "blog/project2/index.html#summary-statistics",
    "title": "Homework 2",
    "section": "2. Summary Statistics",
    "text": "2. Summary Statistics\n\n\nCode\ndf[['SALES_QUANTITY', 'SALES_VALUE']].describe()\n\n\n\n\n\n\n\n\n\nSALES_QUANTITY\nSALES_VALUE\n\n\n\n\ncount\n1.022150e+07\n1.022150e+07\n\n\nmean\n2.433266e+02\n1.871196e+03\n\n\nstd\n2.380844e+03\n3.095331e+05\n\n\nmin\n-2.884200e+05\n-6.898214e+08\n\n\n25%\n2.000000e+01\n1.949600e+02\n\n\n50%\n6.000000e+01\n4.471300e+02\n\n\n75%\n1.200000e+02\n1.014720e+03\n\n\nmax\n1.145520e+06\n6.898287e+08\n\n\n\n\n\n\n\n\n\nCode\n{col: df[col].nunique() for col in ['SKU_ID', 'OUTLET_ID', 'DISTRIBUTOR_ID']}\n\n\n{'SKU_ID': 20096, 'OUTLET_ID': 43529, 'DISTRIBUTOR_ID': 14}"
  },
  {
    "objectID": "blog/project2/index.html#visualizing-distributions",
    "href": "blog/project2/index.html#visualizing-distributions",
    "title": "Homework 2",
    "section": "3. Visualizing Distributions",
    "text": "3. Visualizing Distributions\n\n\nCode\nplt.hist(df['SALES_QUANTITY'], bins=50, log=True)\nplt.title(\"Sales Quantity Distribution\")\nplt.xlabel(\"Quantity\")\nplt.ylabel(\"Log Count\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nfiltered_value = df['SALES_VALUE'].clip(upper=df['SALES_VALUE'].quantile(0.99))\nplt.hist(filtered_value, bins=50, log=True)\nplt.title(\"Sales Value Distribution (Clipped)\")\nplt.xlabel(\"Value\")\nplt.ylabel(\"Log Count\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\ndaily_counts = df['RECORD_DATE'].value_counts().sort_index()\ndaily_counts.plot(figsize=(12,4))\nplt.title(\"Daily Transaction Counts\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Count\")\nplt.show()"
  },
  {
    "objectID": "blog/project2/index.html#top-performing-entities",
    "href": "blog/project2/index.html#top-performing-entities",
    "title": "Homework 2",
    "section": "4. Top Performing Entities",
    "text": "4. Top Performing Entities\n\n\nCode\nfor col in ['DISTRIBUTOR_ID', 'OUTLET_ID', 'SKU_ID']:\n    df[col].value_counts().head(10).plot(kind='bar')\n    plt.title(f\"Top 10 {col}\")\n    plt.ylabel(\"Frequency\")\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.show()"
  },
  {
    "objectID": "blog/project2/index.html#sku-lifecycle-and-discontinuation",
    "href": "blog/project2/index.html#sku-lifecycle-and-discontinuation",
    "title": "Homework 2",
    "section": "5. SKU Lifecycle and Discontinuation",
    "text": "5. SKU Lifecycle and Discontinuation\n\n\nCode\nlatest_date = df['RECORD_DATE'].max()\nsku_lifecycle = df.groupby('SKU_ID')['RECORD_DATE'].agg(['min', 'max', 'count']).reset_index()\nsku_lifecycle.columns = ['SKU_ID', 'Start_Date', 'End_Date', 'Active_Days']\nsku_lifecycle['Days_Since_Last_Sale'] = (latest_date - sku_lifecycle['End_Date']).dt.days\nsku_lifecycle['Potentially_Discontinued'] = sku_lifecycle['Days_Since_Last_Sale'] &gt; 180\nsku_lifecycle.head()\n\n\n\n\n\n\n\n\n\nSKU_ID\nStart_Date\nEnd_Date\nActive_Days\nDays_Since_Last_Sale\nPotentially_Discontinued\n\n\n\n\n0\nA001\n2021-11-28\n2021-11-28\n1\n854\nTrue\n\n\n1\nA002\n2021-01-25\n2024-02-20\n31\n40\nFalse\n\n\n2\nA003\n2021-03-21\n2021-10-22\n7\n891\nTrue\n\n\n3\nA004\n2021-04-29\n2021-04-29\n1\n1067\nTrue\n\n\n4\nA005\n2021-01-11\n2024-03-10\n96\n21\nFalse\n\n\n\n\n\n\n\n\n\nCode\nsns.countplot(x='Potentially_Discontinued', data=sku_lifecycle)\nplt.title(\"Potentially Discontinued SKUs\")\nplt.show()"
  },
  {
    "objectID": "blog/project2/index.html#feature-engineering",
    "href": "blog/project2/index.html#feature-engineering",
    "title": "Homework 2",
    "section": "6. Feature Engineering",
    "text": "6. Feature Engineering\n\n\nCode\ndf['WEEK'] = df['RECORD_DATE'].dt.to_period('W').apply(lambda r: r.start_time)\nweekly_sales = df.pivot_table(index='SKU_ID', columns='WEEK', values='SALES_QUANTITY', aggfunc='sum', fill_value=0)\n\nfeatures = pd.DataFrame(index=weekly_sales.index)\nfeatures['mean'] = weekly_sales.mean(axis=1)\nfeatures['std'] = weekly_sales.std(axis=1)\nfeatures['cv'] = features['std'] / features['mean'].replace(0, np.nan)\nfeatures['max'] = weekly_sales.max(axis=1)\nfeatures['med'] = weekly_sales.median(axis=1)\n\nfeatures = features.join(sku_lifecycle.set_index('SKU_ID')[['Active_Days', 'Days_Since_Last_Sale']])\nfeatures.dropna(inplace=True)\nfeatures.head()\n\n\n\n\n\n\n\n\n\nmean\nstd\ncv\nmax\nmed\nActive_Days\nDays_Since_Last_Sale\n\n\nSKU_ID\n\n\n\n\n\n\n\n\n\n\n\nA001\n0.058824\n0.766965\n13.038405\n10\n0.0\n1\n854\n\n\nA002\n2.058824\n5.205489\n2.528380\n30\n0.0\n31\n40\n\n\nA003\n0.411765\n2.270438\n5.513922\n20\n0.0\n7\n891\n\n\nA004\n0.117647\n1.533930\n13.038405\n20\n0.0\n1\n1067\n\n\nA005\n10.705882\n23.996461\n2.241428\n140\n0.0\n96\n21"
  },
  {
    "objectID": "blog/project2/index.html#kmeans-clustering-with-pca",
    "href": "blog/project2/index.html#kmeans-clustering-with-pca",
    "title": "Homework 2",
    "section": "7. KMeans Clustering with PCA",
    "text": "7. KMeans Clustering with PCA\n\n\nCode\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(features)\n\npca = PCA(n_components=2, random_state=42)\nX_pca = pca.fit_transform(X_scaled)\n\nkmeans = KMeans(n_clusters=3, random_state=42)\nclusters = kmeans.fit_predict(X_pca)\nfeatures['Cluster'] = clusters\n\nsns.scatterplot(x=X_pca[:,0], y=X_pca[:,1], hue=features['Cluster'], palette='tab10')\nplt.title(\"KMeans Clustering (PCA Reduced)\")\nplt.show()"
  },
  {
    "objectID": "blog/project2/index.html#time-series-clustering-with-dtw",
    "href": "blog/project2/index.html#time-series-clustering-with-dtw",
    "title": "Homework 2",
    "section": "8. Time-Series Clustering with DTW",
    "text": "8. Time-Series Clustering with DTW\n\n\nCode\nfrom tslearn.preprocessing import TimeSeriesScalerMeanVariance\nfrom tslearn.clustering import TimeSeriesKMeans\n\nX_time_series = TimeSeriesScalerMeanVariance().fit_transform(weekly_sales.loc[features.index].values)\n\ndtw_kmeans = TimeSeriesKMeans(n_clusters=3, metric='dtw', random_state=42)\nts_clusters = dtw_kmeans.fit_predict(X_time_series)\nfeatures['TS_Cluster'] = ts_clusters\n\nfor i, centroid in enumerate(dtw_kmeans.cluster_centers_):\n    plt.plot(centroid.ravel(), label=f'Cluster {i}')\nplt.title(\"DTW Cluster Centroids\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "blog/project2/index.html#cluster-trend-visualization",
    "href": "blog/project2/index.html#cluster-trend-visualization",
    "title": "Homework 2",
    "section": "9. Cluster Trend Visualization",
    "text": "9. Cluster Trend Visualization\n\n\nCode\nweekly_melted = weekly_sales.reset_index().melt(id_vars='SKU_ID', var_name='Week', value_name='Sales')\nweekly_melted['Week'] = pd.to_datetime(weekly_melted['Week'])\n\nweekly_merged = weekly_melted.merge(features[['Cluster']], left_on='SKU_ID', right_index=True)\nclustered_sales = weekly_merged.groupby(['Cluster', 'Week'])['Sales'].sum().reset_index()\n\nfor cluster in clustered_sales['Cluster'].unique():\n    cluster_data = clustered_sales[clustered_sales['Cluster'] == cluster]\n    plt.plot(cluster_data['Week'], cluster_data['Sales'], label=f'Cluster {cluster}')\n\nplt.title(\"Weekly Sales Quantity by Cluster\")\nplt.xlabel(\"Week\")\nplt.ylabel(\"Total Sales Quantity\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "blog/project2/index.html#so-what-do-we-gain-from-this",
    "href": "blog/project2/index.html#so-what-do-we-gain-from-this",
    "title": "Homework 2",
    "section": "So what do we gain from this?",
    "text": "So what do we gain from this?\n\nFirst, we can now tag SKUs — stable, erratic, or temporary. That helps with planning and inventory.\nSecond, we can target better. Each group might need a different strategy — in pricing, ads, or supply chain.\nThird, we can reduce waste. If we know a product won’t last long, we can act early and avoid excess stock.\nAnd finally, if we combine this with business KPIs — like margin or region — we might find that some clusters do better in specific places or times. That can guide future launches."
  },
  {
    "objectID": "blog/project2/index.html#blueprinty-case-study",
    "href": "blog/project2/index.html#blueprinty-case-study",
    "title": "Homework 2",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years (patents), regional location (region), age since incorporation (age), and whether or not the firm uses Blueprinty’s software (iscustomer). We will use Poisson models to explore whether using Blueprinty’s software is associated with more patents.\n\n\n\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.special import gammaln\nfrom scipy import optimize\nimport statsmodels.api as sm\nfrom statsmodels.iolib.summary2 import summary_col\n\n# Read Blueprinty data\ndf_bp = pd.read_csv(\"blueprinty.csv\")\n\n\n\n\n\n\nCode\nplt.figure(figsize=(8, 5))\nsns.histplot(data=df_bp, x=\"patents\", hue=\"iscustomer\", element=\"step\", stat=\"count\", bins=15, palette=\"Set2\")\nplt.xlabel(\"Number of Patents (last 5 years)\")\nplt.ylabel(\"Count of Firms\")\nplt.title(\"Distribution of Patent Counts by Blueprinty Customer Status\")\nplt.show()\n\nprint(df_bp.groupby('iscustomer')['patents'].mean())\n\n\n\n\n\n\n\n\n\niscustomer\n0    3.473013\n1    4.133056\nName: patents, dtype: float64\n\n\n\nObservation: Blueprinty customers have a higher average patent count, but age and region differences warrant controls.\n\n\n\n\n\n\nCode\nsns.boxplot(data=df_bp, x='iscustomer', y='age', palette='Set2')\nplt.title('Firm Age by Customer Status')\nplt.show()\n\nsns.countplot(data=df_bp, x='region', hue='iscustomer', palette='Set2')\nplt.title('Regional Distribution by Customer Status')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndef loglik_poisson(lmbda, y):\n    return np.sum(y * np.log(lmbda) - lmbda - gammaln(y + 1))\n\ny = df_bp['patents'].values\nlams = np.linspace(0.1, y.mean()*2, 200)\nlls = [loglik_poisson(l, y) for l in lams]\nplt.plot(lams, lls)\nplt.axvline(y.mean(), linestyle='--', color='red')\nplt.show()\nprint(f\"MLE (analytical) = {y.mean():.2f}\")\n\n\n\n\n\n\n\n\n\nMLE (analytical) = 3.68\n\n\n\n\nCode\nres = optimize.minimize_scalar(lambda L: -loglik_poisson(L, y), bounds=(0.1, y.mean()*3), method='bounded')\nprint(f\"MLE via optimize = {res.x:.2f}, success={res.success}\")\n\n\nMLE via optimize = 3.68, success=True\n\n\n\n\n\n\n\nCode\ndf_bp['age_z'] = (df_bp['age'] - df_bp['age'].mean()) / df_bp['age'].std()\ndf_bp['age_sq_z'] = ((df_bp['age']**2) - (df_bp['age']**2).mean()) / (df_bp['age']**2).std()\nX_df = pd.get_dummies(df_bp[['age_z','age_sq_z','region','iscustomer']], drop_first=True)\nX_df.insert(0, 'Intercept', 1)\nX_mat = X_df.astype(float).to_numpy()\ny = df_bp['patents'].values.astype(float)\n\n\n\n\nCode\nfrom scipy.optimize import minimize\ndef negll(beta, y, X):\n    eta = np.clip(X @ beta, -20, 20)\n    mu = np.exp(eta)\n    return -np.sum(y * eta - mu - gammaln(y + 1))\n\ninitial_params = np.zeros(X_mat.shape[1])\nres_reg = minimize(\n    negll,\n    initial_params,\n    args=(y, X_mat),\n    method='BFGS',\n    options={'disp': True, 'gtol': 1e-8, 'maxiter': 5000}\n)\n\nbeta_hat = res_reg.x\nse_hat = np.sqrt(np.diag(res_reg.hess_inv))\nresult_table = pd.DataFrame({'coef': beta_hat, 'se': se_hat}, index=X_df.columns)\nprint(\"Converged:\", res_reg.success)\nprint(result_table)\n\n\n         Current function value: 3258.072145\n         Iterations: 16\n         Function evaluations: 216\n         Gradient evaluations: 24\nConverged: False\n                      coef        se\nIntercept         1.188966  0.032983\nage_z             1.076381  0.100340\nage_sq_z         -1.181943  0.102732\niscustomer        0.207591  0.032223\nregion_Northeast  0.029170  0.038348\nregion_Northwest -0.017574  0.036880\nregion_South      0.056561  0.049465\nregion_Southwest  0.050576  0.042629\n\n\n\n\nCode\n# Built-in statsmodels Poisson\nmodel_bp = sm.GLM(y, X_df.astype(float), family=sm.families.Poisson()).fit()\nprint(model_bp.summary())\n\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                      y   No. Observations:                 1500\nModel:                            GLM   Df Residuals:                     1492\nModel Family:                 Poisson   Df Model:                            7\nLink Function:                    Log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -3258.1\nDate:                Tue, 06 May 2025   Deviance:                       2143.3\nTime:                        23:47:24   Pearson chi2:                 2.07e+03\nNo. Iterations:                     5   Pseudo R-squ. (CS):             0.1360\nCovariance Type:            nonrobust                                         \n====================================================================================\n                       coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------------\nIntercept            1.1890      0.037     32.365      0.000       1.117       1.261\nage_z                1.0764      0.100     10.716      0.000       0.880       1.273\nage_sq_z            -1.1819      0.103    -11.513      0.000      -1.383      -0.981\niscustomer           0.2076      0.031      6.719      0.000       0.147       0.268\nregion_Northeast     0.0292      0.044      0.669      0.504      -0.056       0.115\nregion_Northwest    -0.0176      0.054     -0.327      0.744      -0.123       0.088\nregion_South         0.0566      0.053      1.074      0.283      -0.047       0.160\nregion_Southwest     0.0506      0.047      1.072      0.284      -0.042       0.143\n====================================================================================\n\n\n\n\n\nThe Poisson regression model yields several important insights:\n\nFirm Age: The coefficients on age_z and age_sq_z suggest a nonlinear relationship: patent counts increase with firm age up to a point, then decline. This inverted-U pattern may reflect a life-cycle effect.\nBlueprinty Customers: The key coefficient for iscustomer is approximately 0.21, and statistically significant. The exponentiated value, exp(0.21) ≈ 1.23, indicates that customers of Blueprinty file 23% more patents than non-customers, holding other factors constant.\nRegion Variables: All region dummy coefficients are small and not statistically significant, implying that regional differences are minimal when other factors are controlled for.\n\nThese results align with the observed group means and validate the firm’s marketing claim to some extent, though we caution that causality is not guaranteed.\n\n\nCode\n# Marginal effect for iscustomer\n# Ensure column order matches model training data\nX_base = X_df.copy().astype(float)\nX0 = X_base.copy()\nX1 = X_base.copy()\n\n# Set iscustomer dummy column\niscustomer_col = [col for col in X_base.columns if 'iscustomer' in col]\nif iscustomer_col:\n    col = iscustomer_col[0]\n    X0[col] = 0\n    X1[col] = 1\n\n# Ensure column alignment\nX0 = X0[model_bp.params.index]\nX1 = X1[model_bp.params.index]\n\n# Predict\npred0 = model_bp.predict(X0)\npred1 = model_bp.predict(X1)\nprint(\"Avg patent increase for Blueprinty customers:\", (pred1 - pred0).mean())\n\n\nAvg patent increase for Blueprinty customers: 0.7927680710453309\n\n\n\nConclusion: Controlling for age and region, Blueprinty customers average about 1.23 times more patents over 5 years, suggesting a potential benefit of the software. Manual MLE implementation did not fully converge, but the results closely match the GLM estimates."
  },
  {
    "objectID": "blog/project2/index.html#airbnb-case-study",
    "href": "blog/project2/index.html#airbnb-case-study",
    "title": "Homework 2",
    "section": "Airbnb Case Study",
    "text": "Airbnb Case Study\n\nData & EDA\n\n\nCode\ndf_ab = pd.read_csv(\"airbnb.csv\", parse_dates=['last_scraped','host_since'])\ndf_ab['days_listed'] = (df_ab['last_scraped'] - df_ab['host_since']).dt.days\nvars_keep = ['number_of_reviews','room_type','days_listed','bathrooms','bedrooms','price',\n             'review_scores_cleanliness','review_scores_location','review_scores_value','instant_bookable']\ndf_ab = df_ab.dropna(subset=vars_keep)\nplt.hist(df_ab['number_of_reviews'], bins=30)\nplt.title('Review Count Distribution')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression for Reviews\n\n\nCode\ndf_ab['clean'] = df_ab['review_scores_cleanliness']\nX_ab = pd.get_dummies(df_ab[['days_listed','room_type','bathrooms','bedrooms','price','clean',\n                              'review_scores_location','review_scores_value','instant_bookable']], drop_first=True)\nX_ab.insert(0,'Intercept',1)\nX_ab = X_ab.astype(float)\ny_ab = df_ab['number_of_reviews'].values\nmodel_ab = sm.GLM(y_ab, X_ab, family=sm.families.Poisson()).fit()\nprint(summary_col([model_ab], float_format='%.3f', stars=True))\n\n\n\n================================\n                           y    \n--------------------------------\nIntercept              2.943*** \n                       (0.017)  \ndays_listed            0.001*** \n                       (0.000)  \nbathrooms              -0.113***\n                       (0.004)  \nbedrooms               0.076*** \n                       (0.002)  \nprice                  -0.000***\n                       (0.000)  \nclean                  0.111*** \n                       (0.002)  \nreview_scores_location -0.081***\n                       (0.002)  \nreview_scores_value    -0.091***\n                       (0.002)  \nroom_type_Private room 0.019*** \n                       (0.003)  \nroom_type_Shared room  -0.115***\n                       (0.009)  \ninstant_bookable_t     0.459*** \n                       (0.003)  \n================================\nStandard errors in parentheses.\n* p&lt;.1, ** p&lt;.05, ***p&lt;.01\n\n\n\n\nAnalysis of Airbnb Regression Results\nThe Poisson regression for Airbnb reviews reveals several clear patterns:\n\nDays Listed: Listings that have been active for more days receive significantly more reviews, consistent with cumulative exposure.\nRoom Type: Entire-home listings get more reviews than shared/private rooms, likely due to higher demand and broader appeal.\nInstant Bookable: This convenience feature is positively associated with review count, indicating user preference for low-friction booking experiences.\nCleanliness, Location, and Value Scores: Higher review sub-scores positively correlate with the number of reviews, suggesting a feedback loop between guest satisfaction and visibility.\n\nOverall, the model identifies actionable levers for hosts seeking to increase guest engagement through reviews."
  }
]